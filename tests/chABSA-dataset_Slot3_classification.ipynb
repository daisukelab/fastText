{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastText chABSA dataset slot 3 evaluation\n",
    "\n",
    "Tested single data split only, not cross validation applied.\n",
    "\n",
    "### Polarity classification\n",
    "\n",
    "    P@1\t0.751\n",
    "    R@1\t0.751\n",
    "    Number of examples: 265\n",
    "\n",
    "### Category classification\n",
    "\n",
    "    P@1\t0.464\n",
    "    R@1\t0.464\n",
    "    Number of examples: 265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'dl-cliche'...\n",
      "remote: Enumerating objects: 141, done.\u001b[K\n",
      "remote: Counting objects: 100% (141/141), done.\u001b[K\n",
      "remote: Compressing objects: 100% (100/100), done.\u001b[K\n",
      "remote: Total 141 (delta 83), reused 93 (delta 40), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (141/141), 31.97 KiB | 0 bytes/s, done.\n",
      "Resolving deltas: 100% (83/83), done.\n",
      "Checking connectivity... done.\n",
      "Processing /home/xxxx/lab/NLP/bert/dl-cliche\n",
      "Requirement already satisfied: numpy in /home/xxxx/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages (from dl-cliche==0.0.1) (1.15.2)\n",
      "Building wheels for collected packages: dl-cliche\n",
      "  Running setup.py bdist_wheel for dl-cliche ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/xxxx/.cache/pip/wheels/75/2e/09/2ade8d86933e056eb5238298b5b39f70d99ba03f4b9ae35df6\n",
      "Successfully built dl-cliche\n",
      "Installing collected packages: dl-cliche\n",
      "  Found existing installation: dl-cliche 0.0.1\n",
      "    Uninstalling dl-cliche-0.0.1:\n",
      "      Successfully uninstalled dl-cliche-0.0.1\n",
      "Successfully installed dl-cliche-0.0.1\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/daisukelab/dl-cliche.git\n",
    "! cd dl-cliche && pip install .\n",
    "! rm -fr dl-cliche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlcliche.utils import *\n",
    "from dlcliche.nlp_mecab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-11-09 13:18:57--  https://s3-ap-northeast-1.amazonaws.com/dev.tech-sketch.jp/chakki/public/chABSA-dataset.zip\n",
      "Resolving s3-ap-northeast-1.amazonaws.com (s3-ap-northeast-1.amazonaws.com)... 52.219.0.80\n",
      "Connecting to s3-ap-northeast-1.amazonaws.com (s3-ap-northeast-1.amazonaws.com)|52.219.0.80|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 722777 (706K) [application/zip]\n",
      "Saving to: ‘chABSA-dataset.zip’\n",
      "\n",
      "chABSA-dataset.zip  100%[===================>] 705.84K  --.-KB/s    in 0.09s   \n",
      "\n",
      "2018-11-09 13:18:57 (7.88 MB/s) - ‘chABSA-dataset.zip’ saved [722777/722777]\n",
      "\n",
      "e00008_ann.json  e01118_ann.json  e02289_ann.json  e04291_ann.json\n",
      "e00017_ann.json  e01151_ann.json  e02353_ann.json  e04298_ann.json\n",
      "e00024_ann.json  e01156_ann.json  e02367_ann.json  e04304_ann.json\n",
      "e00026_ann.json  e01173_ann.json  e02380_ann.json  e04319_ann.json\n",
      "e00030_ann.json  e01183_ann.json  e02382_ann.json  e04329_ann.json\n",
      "e00033_ann.json  e01197_ann.json  e02390_ann.json  e04331_ann.json\n",
      "e00034_ann.json  e01216_ann.json  e02414_ann.json  e04360_ann.json\n",
      "e00035_ann.json  e01230_ann.json  e02423_ann.json  e04503_ann.json\n",
      "e00037_ann.json  e01244_ann.json  e02505_ann.json  e04509_ann.json\n",
      "e00051_ann.json  e01249_ann.json  e02525_ann.json  e04727_ann.json\n",
      "e00053_ann.json  e01260_ann.json  e02530_ann.json  e04768_ann.json\n",
      "e00058_ann.json  e01334_ann.json  e02544_ann.json  e04844_ann.json\n",
      "e00069_ann.json  e01364_ann.json  e02547_ann.json  e04856_ann.json\n",
      "e00091_ann.json  e01398_ann.json  e02563_ann.json  e04858_ann.json\n",
      "e00107_ann.json  e01402_ann.json  e02567_ann.json  e04867_ann.json\n",
      "e00114_ann.json  e01425_ann.json  e02608_ann.json  e04877_ann.json\n",
      "e00146_ann.json  e01436_ann.json  e02627_ann.json  e04948_ann.json\n",
      "e00168_ann.json  e01462_ann.json  e02632_ann.json  e04976_ann.json\n",
      "e00184_ann.json  e01469_ann.json  e02643_ann.json  e04995_ann.json\n",
      "e00194_ann.json  e01506_ann.json  e02673_ann.json  e05011_ann.json\n",
      "e00273_ann.json  e01528_ann.json  e02682_ann.json  e05110_ann.json\n",
      "e00308_ann.json  e01529_ann.json  e02732_ann.json  e05145_ann.json\n",
      "e00343_ann.json  e01533_ann.json  e02825_ann.json  e05155_ann.json\n",
      "e00354_ann.json  e01546_ann.json  e02837_ann.json  e05167_ann.json\n",
      "e00380_ann.json  e01585_ann.json  e02889_ann.json  e05302_ann.json\n",
      "e00406_ann.json  e01620_ann.json  e02905_ann.json  e05319_ann.json\n",
      "e00435_ann.json  e01624_ann.json  e02946_ann.json  e05322_ann.json\n",
      "e00457_ann.json  e01629_ann.json  e03128_ann.json  e05346_ann.json\n",
      "e00465_ann.json  e01635_ann.json  e03226_ann.json  e05452_ann.json\n",
      "e00501_ann.json  e01703_ann.json  e03236_ann.json  e05469_ann.json\n",
      "e00534_ann.json  e01719_ann.json  e03267_ann.json  e05480_ann.json\n",
      "e00541_ann.json  e01731_ann.json  e03275_ann.json  e05593_ann.json\n",
      "e00547_ann.json  e01740_ann.json  e03281_ann.json  e05629_ann.json\n",
      "e00563_ann.json  e01743_ann.json  e03367_ann.json  e05714_ann.json\n",
      "e00603_ann.json  e01764_ann.json  e03398_ann.json  e05737_ann.json\n",
      "e00686_ann.json  e01794_ann.json  e03401_ann.json  e07801_ann.json\n",
      "e00693_ann.json  e01798_ann.json  e03472_ann.json  e21200_ann.json\n",
      "e00694_ann.json  e01813_ann.json  e03505_ann.json  e21261_ann.json\n",
      "e00721_ann.json  e01849_ann.json  e03556_ann.json  e23818_ann.json\n",
      "e00772_ann.json  e01862_ann.json  e03566_ann.json  e25665_ann.json\n",
      "e00787_ann.json  e01865_ann.json  e03580_ann.json  e26332_ann.json\n",
      "e00810_ann.json  e01903_ann.json  e03582_ann.json  e26443_ann.json\n",
      "e00832_ann.json  e01904_ann.json  e03584_ann.json  e26454_ann.json\n",
      "e00838_ann.json  e01933_ann.json  e03601_ann.json  e26713_ann.json\n",
      "e00840_ann.json  e01946_ann.json  e03641_ann.json  e26914_ann.json\n",
      "e00842_ann.json  e01968_ann.json  e03693_ann.json  e27050_ann.json\n",
      "e00858_ann.json  e01972_ann.json  e03723_ann.json  e27633_ann.json\n",
      "e00877_ann.json  e01974_ann.json  e03784_ann.json  e27759_ann.json\n",
      "e00886_ann.json  e01987_ann.json  e03929_ann.json  e30066_ann.json\n",
      "e00909_ann.json  e01992_ann.json  e03991_ann.json  e30085_ann.json\n",
      "e00911_ann.json  e02009_ann.json  e04078_ann.json  e30479_ann.json\n",
      "e00939_ann.json  e02049_ann.json  e04091_ann.json  e30746_ann.json\n",
      "e00962_ann.json  e02105_ann.json  e04123_ann.json  e32161_ann.json\n",
      "e00976_ann.json  e02150_ann.json  e04136_ann.json  e32189_ann.json\n",
      "e01018_ann.json  e02152_ann.json  e04147_ann.json  e32458_ann.json\n",
      "e01043_ann.json  e02214_ann.json  e04191_ann.json  e33009_ann.json\n",
      "e01054_ann.json  e02241_ann.json  e04242_ann.json\n",
      "e01097_ann.json  e02246_ann.json  e04273_ann.json\n",
      "--2018-11-09 13:18:58--  https://raw.githubusercontent.com/chakki-works/chABSA-dataset/master/notebooks/resource/stop_words_ja.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.108.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1851 (1.8K) [text/plain]\n",
      "Saving to: ‘stop_words_ja.txt’\n",
      "\n",
      "stop_words_ja.txt   100%[===================>]   1.81K  --.-KB/s    in 0s      \n",
      "\n",
      "2018-11-09 13:18:58 (235 MB/s) - ‘stop_words_ja.txt’ saved [1851/1851]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download dataset & stop_words_ja.txt\n",
    "! wget https://s3-ap-northeast-1.amazonaws.com/dev.tech-sketch.jp/chakki/public/chABSA-dataset.zip\n",
    "! unzip -q chABSA-dataset.zip && rm chABSA-dataset.zip && rm -r __MACOSX\n",
    "! ls chABSA-dataset\n",
    "! cd chABSA-dataset && wget https://raw.githubusercontent.com/chakki-works/chABSA-dataset/master/notebooks/resource/stop_words_ja.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230 files ready.\n",
      "310 stop words ready.\n"
     ]
    }
   ],
   "source": [
    "DATA = Path('chABSA-dataset')\n",
    "\n",
    "def check_data_existence(folder):\n",
    "    file_count = len(list(folder.glob(\"e*_ann.json\")))\n",
    "    if  file_count == 0:\n",
    "        raise Exception(\"Processed Data does not exist.\")\n",
    "    else:\n",
    "        print(\"{} files ready.\".format(file_count))\n",
    "\n",
    "check_data_existence(DATA)\n",
    "\n",
    "stop_words = []\n",
    "with (DATA/\"stop_words_ja.txt\").open(encoding=\"utf-8\") as f:\n",
    "    stop_words = f.readlines()\n",
    "    stop_words = [w.strip() for w in stop_words]\n",
    "\n",
    "print(\"{} stop words ready.\".format(len(stop_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['market#general', 'company#general', 'company#sales', 'company#profit', 'company#amount', 'company#price', 'company#cost', 'business#general', 'business#sales', 'business#profit', 'business#amount', 'business#price', 'business#cost', 'product#general', 'product#sales', 'product#profit', 'product#amount', 'product#price', 'product#cost']\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "# make labels (exclude NULL and OOD)\n",
    "for e in [\"market\", \"company\", \"business\", \"product\"]:\n",
    "    for a in [\"general\", \"sales\", \"profit\", \"amount\", \"price\", \"cost\"]:\n",
    "        labels.append(e + \"#\" + a)\n",
    "        if e in [\"market\"]:\n",
    "            break;\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "sentences = []\n",
    "dataset = []\n",
    "tokenizer = get_mecab_tokenizer(stop_words=stop_words, normalize=False)\n",
    "\n",
    "for f in DATA.glob(\"e*_ann.json\"):\n",
    "    with f.open(encoding=\"utf-8\") as j:\n",
    "        d = json.load(j)\n",
    "        for s in d[\"sentences\"]:\n",
    "            tokenized = tokenizer.tokenize(s[\"sentence\"].upper())\n",
    "            for o in s[\"opinions\"]:\n",
    "                if o[\"category\"] in labels:\n",
    "                    # sentence index + category\n",
    "                    dataset.append((len(sentences), o[\"category\"], o[\"polarity\"]))\n",
    "            sentences.append(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polarity classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2381, 265, ['negative', 'neutral', 'positive'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Y = 2\n",
    "dataset = np.array(dataset)\n",
    "Xtrn, Xval, ytrn, yval = train_test_split(dataset[:, 0], dataset[:, Y], test_size=0.1, random_state=0)\n",
    "\n",
    "def write_dataset(filename, X, y):\n",
    "    with open(filename, 'w') as f:\n",
    "        for _x, _y in zip(X, y):\n",
    "            w = ['__label__'+_y]\n",
    "            w += list(sentences[int(_x)])\n",
    "            f.write(' '.join(w)+'\\n')\n",
    "write_dataset(DATA/'train.txt', Xtrn, ytrn)\n",
    "write_dataset(DATA/'valid.txt', Xval, yval)\n",
    "len(Xtrn), len(Xval), list(set(dataset[:, Y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  3705\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread: 2769147 lr:  0.000000 loss:  0.209850 ETA:   0h 0m 0.237838 ETA:   0h 0m  0h 0m\n"
     ]
    }
   ],
   "source": [
    "! fasttext supervised -input {DATA}/train.txt -output {DATA}/ft_supervised -dim 50 -epoch 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t265\r\n",
      "P@1\t0.751\r\n",
      "R@1\t0.751\r\n",
      "Number of examples: 265\r\n"
     ]
    }
   ],
   "source": [
    "! fasttext test {DATA}/ft_supervised.bin {DATA}/valid.txt 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2381,\n",
       " 265,\n",
       " ['product#profit',\n",
       "  'business#price',\n",
       "  'company#general',\n",
       "  'business#profit',\n",
       "  'business#amount',\n",
       "  'product#amount',\n",
       "  'product#price',\n",
       "  'product#general',\n",
       "  'company#amount',\n",
       "  'business#sales',\n",
       "  'company#cost',\n",
       "  'company#profit',\n",
       "  'market#general',\n",
       "  'business#cost',\n",
       "  'product#cost',\n",
       "  'business#general',\n",
       "  'company#sales',\n",
       "  'product#sales'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Y = 1\n",
    "dataset = np.array(dataset)\n",
    "Xtrn, Xval, ytrn, yval = train_test_split(dataset[:, 0], dataset[:, Y], test_size=0.1, random_state=0)\n",
    "\n",
    "def write_dataset(filename, X, y):\n",
    "    with open(filename, 'w') as f:\n",
    "        for _x, _y in zip(X, y):\n",
    "            w = ['__label__'+_y]\n",
    "            w += list(sentences[int(_x)])\n",
    "            f.write(' '.join(w)+'\\n')\n",
    "write_dataset(DATA/'train.txt', Xtrn, ytrn)\n",
    "write_dataset(DATA/'valid.txt', Xval, yval)\n",
    "len(Xtrn), len(Xval), list(set(dataset[:, Y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  3705\n",
      "Number of labels: 18\n",
      "Progress: 100.0% words/sec/thread: 2313320 lr:  0.000000 loss:  0.431946 ETA:   0h 0m 0.457045 ETA:   0h 0m 0.439391 ETA:   0h 0mh 0m\n"
     ]
    }
   ],
   "source": [
    "! fasttext supervised -input {DATA}/train.txt -output {DATA}/ft_supervised -dim 50 -epoch 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t265\r\n",
      "P@1\t0.464\r\n",
      "R@1\t0.464\r\n",
      "Number of examples: 265\r\n"
     ]
    }
   ],
   "source": [
    "! fasttext test {DATA}/ft_supervised.bin {DATA}/valid.txt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
